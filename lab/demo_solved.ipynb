{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you do not have the necessary packages or libraries installed, execute these commands.\n",
    "#!pip install torchvision\n",
    "#!conda install pytorch -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries or packages\n",
    "from math import floor\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MNIST dataset for training and testing\n",
    "# training MNIST data\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=ToTensor())\n",
    "# testing MNIST data\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the type of training data and the shape of training data (images) and targets (labels)\n",
    "print(type(train_data.data))\n",
    "print(train_data.data.shape)\n",
    "print(train_data.targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network structure (3 layers fully connected network)\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, 500)#input layer\n",
    "        self.fc2 = nn.Linear(500, 300)#hidden layer\n",
    "        self.output = nn.Linear(300, 2)#output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))# activation function is relu\n",
    "        x = F.relu(self.fc2(x))# activation function is relu\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of MNIST data is 28*28=784\n",
    "input_shape = 784\n",
    "# training hyperparameters\n",
    "n_epoch = 2\n",
    "learning_rate = 0.001\n",
    "minibatch_sz = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network, optimizer and define the loss function\n",
    "network = Network(input_shape)\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will perfom task-wise training. A single task comprises of two classes from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1 = [0, 1]#define task 1 classify digits '0' and '1'\n",
    "task2 = [2, 3]#define task 2 classify digits '2' and '3'\n",
    "task3 = [4, 5]#define task 3 classify digits '4' and '5' \n",
    "task4 = [6, 7]#define task 4 classify digits '6' and '7' \n",
    "task5 = [8, 9]#define task 5 classify digits '8' and '9'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separate training and testing samples from each task. This is easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create training sample indexes for each task\n",
    "task1_tr_samples = torch.where(torch.bitwise_or(train_data.targets == task1[0], train_data.targets == task1[1]))[0] #indexs of training samples '0' and '1'\n",
    "task2_tr_samples = torch.where(torch.bitwise_or(train_data.targets == task2[0], train_data.targets == task2[1]))[0] #indexs of training samples '2' and '3'\n",
    "task3_tr_samples = torch.where(torch.bitwise_or(train_data.targets == task3[0], train_data.targets == task3[1]))[0] #indexs of training samples '4' and '5'\n",
    "task4_tr_samples = torch.where(torch.bitwise_or(train_data.targets == task4[0], train_data.targets == task4[1]))[0] #indexs of training samples '6' and '7'\n",
    "task5_tr_samples = torch.where(torch.bitwise_or(train_data.targets == task5[0], train_data.targets == task5[1]))[0] #indexs of training samples '8' and '9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create testing sample indexes for each task\n",
    "task1_ts_samples = torch.where(torch.bitwise_or(test_data.targets == task1[0], test_data.targets == task1[1]))[0] #indexs of testing samples '0' and '1'\n",
    "task2_ts_samples = torch.where(torch.bitwise_or(test_data.targets == task2[0], test_data.targets == task2[1]))[0] #indexs of testing samples '2' and '3'\n",
    "task3_ts_samples = torch.where(torch.bitwise_or(test_data.targets == task3[0], test_data.targets == task3[1]))[0] #indexs of testing samples '4' and '5'\n",
    "task4_ts_samples = torch.where(torch.bitwise_or(test_data.targets == task4[0], test_data.targets == task4[1]))[0] #indexs of testing samples '6' and '7'\n",
    "task5_ts_samples = torch.where(torch.bitwise_or(test_data.targets == task5[0], test_data.targets == task5[1]))[0] #indexs of testing samples '8' and '9'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 1**: The purpose of this question is to demonstrate the problem of catastrophic forgetting. For this purpose, we will train a single network on two different tasks in a sequence. After training evaluate the performance of the trained network on both tasks. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on task 1\n",
    "for e in range(n_epoch):\n",
    "    #calcuate the total number of batch. floor can round a real number x down to the nearest integer that is less than or equal to x\n",
    "    n_batch = floor(task1_tr_samples.shape[0] / minibatch_sz)\n",
    "    for b in range(n_batch):\n",
    "        x_batch = train_data.data[task1_tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]] #get training images\n",
    "        y_batch = train_data.targets[task1_tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]] #get training labels\n",
    "        # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "        x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "        # convert label to one hot\n",
    "        y_batch = F.one_hot(y_batch).float()\n",
    "        # get the prediction\n",
    "        y_hat_batch = network(x_batch)\n",
    "        # calculate the loss\n",
    "        loss = criterion(y_hat_batch, y_batch)\n",
    "        # execute backpropagation\n",
    "        loss.backward()\n",
    "        # update the model\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {e}: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on Task 1\n",
    "n_batch = floor(task1_ts_samples.shape[0] / minibatch_sz) #calcuate the total number of batch\n",
    "n_correct = 0    \n",
    "for b in range(n_batch):\n",
    "    x_batch = test_data.data[task1_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]] #get testing images\n",
    "    y_batch = test_data.targets[task1_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]] #get testing labels\n",
    "    # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "    x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "    # get the prediction\n",
    "    y_hat_batch = network(x_batch)\n",
    "    # get the indices of the maximum values along dimension 1\n",
    "    _, prediction = torch.max(y_hat_batch, 1)\n",
    "    # count the number of correct predictions\n",
    "    n_correct += (prediction == y_batch).sum().item()\n",
    "print(f'Accuracy = {(n_correct * 100) / task1_ts_samples.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on task 2\n",
    "for e in range(n_epoch):\n",
    "    n_batch = floor(task2_tr_samples.shape[0] / minibatch_sz) #calcuate the total number of batch\n",
    "    for b in range(n_batch):\n",
    "        x_batch = train_data.data[task2_tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]] #get training images\n",
    "        y_batch = train_data.targets[task2_tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]] #get training labels\n",
    "        # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "        x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "        #even numbers [0,2,4,6,8] are labeled 0，Odd numbers [1,3,5,7,9] are labeled 1 \n",
    "        y_batch = y_batch % 2\n",
    "        # convert label to one hot\n",
    "        y_batch = F.one_hot(y_batch).float()\n",
    "        # get the prediction\n",
    "        y_hat_batch = network(x_batch)\n",
    "        # calculate the loss\n",
    "        loss = criterion(y_hat_batch, y_batch)\n",
    "        # execute backpropagation\n",
    "        loss.backward()\n",
    "        # update the model\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {e}: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on Task 2\n",
    "n_batch = floor(task2_ts_samples.shape[0] / minibatch_sz) #calcuate the total number of batch\n",
    "n_correct = 0    \n",
    "for b in range(n_batch):\n",
    "    x_batch = test_data.data[task2_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]] #get testing images\n",
    "    y_batch = test_data.targets[task2_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]] #get testing labels\n",
    "    # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "    x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "    # get the prediction\n",
    "    y_hat_batch = network(x_batch)\n",
    "    # get the indices of the maximum values along dimension 1\n",
    "    _, prediction = torch.max(y_hat_batch, 1)\n",
    "    # count the number of correct predictions\n",
    "    n_correct += (prediction == (y_batch % 2)).sum().item()\n",
    "print(f'Accuracy = {(n_correct * 100) / task1_ts_samples.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on Task 1\n",
    "n_batch = floor(task1_ts_samples.shape[0] / minibatch_sz)#calcuate the total number of batch\n",
    "n_correct = 0    \n",
    "for b in range(n_batch):\n",
    "    x_batch = test_data.data[task1_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]] #get testing images\n",
    "    y_batch = test_data.targets[task1_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]] #get testing labels\n",
    "    # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "    x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "    # get the prediction\n",
    "    y_hat_batch = network(x_batch)\n",
    "    # get the indices of the maximum values along dimension 1\n",
    "    _, prediction = torch.max(y_hat_batch, 1)\n",
    "    # count the number of correct predictions\n",
    "    n_correct += (prediction == y_batch).sum().item()\n",
    "print(f'Accuracy = {(n_correct * 100) / task1_ts_samples.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**: The purpose of this question is to study the effect of replay on catatophic forgetting. In this question also, we will train the network on two tasks in a sequence? When we train the network on the second task, we will also use some samples from the first task for replay. To keep things simple, select a random proportaion (say 50%) of samples from the first task for replay. After training evaluate the performance of the trained network on both tasks. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save some samples from previous tasks for replay\n",
    "prop_saved = 0.5 # proportion of samples saved from a task for replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network and optimizer\n",
    "network = Network(input_shape)\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on task 1\n",
    "for e in range(n_epoch):\n",
    "    n_batch = floor(task1_tr_samples.shape[0] / minibatch_sz)#calcuate the total number of batch\n",
    "    for b in range(n_batch):\n",
    "        x_batch = train_data.data[task1_tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]] #get training images\n",
    "        y_batch = train_data.targets[task1_tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]] #get training labels\n",
    "        # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "        x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "        # convert label to one hot\n",
    "        y_batch = F.one_hot(y_batch).float()\n",
    "        # get the prediction\n",
    "        y_hat_batch = network(x_batch)\n",
    "        # calculate the loss\n",
    "        loss = criterion(y_hat_batch, y_batch)\n",
    "        # execute backpropagation\n",
    "        loss.backward()\n",
    "        # update the model\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {e}: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly choose indexes from task 1 training data \n",
    "task1_replay = np.random.choice(task1_tr_samples.numpy(), int(prop_saved * task1_tr_samples.shape[0]))\n",
    "task1_replay_samples = torch.Tensor(task1_replay).int() #numpy convert to tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on task 2 with replay\n",
    "\n",
    "# concatenate samples from task 2 and replay samples from task 1\n",
    "tr_samples = torch.concatenate([task2_tr_samples, task1_replay_samples], dim=0) \n",
    "# randomize the array to mix samples from task 2 and replay\n",
    "np.random.shuffle(tr_samples.numpy())\n",
    "#calcuate the total number of batch\n",
    "n_batch = floor(tr_samples.shape[0] / minibatch_sz)\n",
    "for e in range(n_epoch):    \n",
    "    for b in range(n_batch):\n",
    "        x_batch = train_data.data[tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]] #get training images\n",
    "        y_batch = train_data.targets[tr_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]] #get training labels\n",
    "        # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "        x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "        #even numbers [0,2,4,6,8] are labeled 0，Odd numbers [1,3,5,7,9] are labeled 1 \n",
    "        y_batch = y_batch % 2\n",
    "        # convert label to one hot\n",
    "        y_batch = F.one_hot(y_batch).float()\n",
    "        # get the prediction\n",
    "        y_hat_batch = network(x_batch)\n",
    "        # calculate the loss\n",
    "        loss = criterion(y_hat_batch, y_batch)\n",
    "        # execute backpropagation\n",
    "        loss.backward()\n",
    "        # update the model\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {e}: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on Task 1\n",
    "n_batch = floor(task1_ts_samples.shape[0] / minibatch_sz)#calcuate the total number of batch\n",
    "n_correct = 0    \n",
    "for b in range(n_batch):\n",
    "    x_batch = test_data.data[task1_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]#get testing images\n",
    "    y_batch = test_data.targets[task1_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]#get testing labels\n",
    "    # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "    x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "    # get the prediction\n",
    "    y_hat_batch = network(x_batch)\n",
    "    # get the indices of the maximum values along dimension 1\n",
    "    _, prediction = torch.max(y_hat_batch, 1)\n",
    "    # count the number of correct predictions\n",
    "    n_correct += (prediction == y_batch).sum().item()\n",
    "print(f'Accuracy = {(n_correct * 100) / task1_ts_samples.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on Task 2\n",
    "n_batch = floor(task2_ts_samples.shape[0] / minibatch_sz)\n",
    "n_correct = 0    \n",
    "for b in range(n_batch):\n",
    "    x_batch = test_data.data[task2_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]#get testing images\n",
    "    y_batch = test_data.targets[task2_ts_samples[(b*minibatch_sz):((b+1)*minibatch_sz)]]#get testing labels\n",
    "    # flatten image before presenting to the network and normalize intensities to the range [0, 1]\n",
    "    x_batch = torch.flatten(x_batch / 255, start_dim=1)\n",
    "    # get the prediction\n",
    "    y_hat_batch = network(x_batch)\n",
    "    # get the indices of the maximum values along dimension 1\n",
    "    _, prediction = torch.max(y_hat_batch, 1)\n",
    "    # count the number of correct predictions\n",
    "    n_correct += (prediction == (y_batch % 2)).sum().item()\n",
    "print(f'Accuracy = {(n_correct * 100) / task1_ts_samples.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Directions for further exploration**\n",
    "We will not share solutions for these questions.\n",
    "\n",
    "**Q1**: How does the proportion of samples saved for replay affect the model's performance?\n",
    "\n",
    "**Q2**: Use replay to train the network on more than two tasks. What is the impact of replay on the memory used by your models? Note that replay-based approach requires that you save the replay samples from previous task forever. This implies that the memory required to store samples contributes to your models memory footprint.\n",
    "\n",
    "**Q3**: Can we choose replay samples more smartly so that we generate maximal impact while using minimal memory? For instance, can you use the network's prediction on a given task to identify samples stored for replay?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
